xgb_mae <- mae(xgb_predictions, test_data$RICE_YIELD)
xgb_mse <- mse(xgb_predictions, test_data$RICE_YIELD)
xgb_rmse <- rmse(xgb_predictions, test_data$RICE_YIELD)
xgb_r_squared <- cor(xgb_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for XGBoost
print("XGBoost Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", xgb_mae))
print(paste("Mean Squared Error (MSE):", xgb_mse))
print(paste("Root Mean Squared Error (RMSE):", xgb_rmse))
print(paste("R-squared:", xgb_r_squared))
# Load required libraries
library(knitr)
library(ggplot2)
library(plotly)
install.packages("plotly")
library(plotly)
library(plotly)
library(randomForest)
library(Metrics)
library(ggplot2)
# Read the data
data <- read.csv("X1.csv", header = TRUE)
y_data <- read.csv("y1.csv")
# Remove columns where all values are zero
data_filtered <- data[, colSums(data != 0) > 0]
# Combine data with target variable
data_combined <- cbind(data_filtered, RICE_YIELD = y_data)
# Set seed for reproducibility
set.seed(123)
# Sample indices for training data
train_indices <- sample(1:nrow(data_combined), 0.8 * nrow(data_combined))
# Split data into training and test sets
train_data <- data_combined[train_indices, ]
test_data <- data_combined[-train_indices, ]
# Train random forest model
rf_model <- randomForest(RICE_YIELD ~ ., data = train_data)
# Make predictions
predictions_rf <- predict(rf_model, test_data)
# Calculate performance metrics for Random Forest
mae_rf <- mae(predictions_rf, test_data$RICE_YIELD)
mse_rf <- mse(predictions_rf, test_data$RICE_YIELD)
rmse_rf <- rmse(predictions_rf, test_data$RICE_YIELD)
r_squared_rf <- cor(predictions_rf, test_data$RICE_YIELD)^2
# Print performance metrics for Random Forest
print("Random Forest Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", mae_rf))
print(paste("Mean Squared Error (MSE):", mse_rf))
print(paste("Root Mean Squared Error (RMSE):", rmse_rf))
print(paste("R-squared:", r_squared_rf))
# Train decision tree model
dt_model <- rpart(RICE_YIELD ~ ., data = train_data)
dt_predictions <- predict(dt_model, test_data)
# Calculate performance metrics for Decision Tree
dt_mae <- mae(dt_predictions, test_data$RICE_YIELD)
dt_mse <- mse(dt_predictions, test_data$RICE_YIELD)
dt_rmse <- rmse(dt_predictions, test_data$RICE_YIELD)
dt_r_squared <- cor(dt_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for Decision Tree
print("Decision Tree Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", dt_mae))
print(paste("Mean Squared Error (MSE):", dt_mse))
print(paste("Root Mean Squared Error (RMSE):", dt_rmse))
print(paste("R-squared:", dt_r_squared))
# Train SVM model
svm_model <- svm(RICE_YIELD ~ ., data = train_data)
# Predictions using SVM model
svm_predictions <- predict(svm_model, test_data)
# Calculate performance metrics for SVM
svm_mae <- mae(svm_predictions, test_data$RICE_YIELD)
svm_mse <- mse(svm_predictions, test_data$RICE_YIELD)
svm_rmse <- rmse(svm_predictions, test_data$RICE_YIELD)
svm_r_squared <- cor(svm_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for SVM
print("SVM Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", svm_mae))
print(paste("Mean Squared Error (MSE):", svm_mse))
print(paste("Root Mean Squared Error (RMSE):", svm_rmse))
print(paste("R-squared:", svm_r_squared))
# Train XGBoost model
xgb_model <- xgboost(data = as.matrix(train_data[, -which(names(train_data) == "RICE_YIELD")]),
label = train_data$RICE_YIELD,
nrounds = 100,  # Number of boosting rounds
objective = "reg:squarederror",  # Objective for regression
eval_metric = "rmse")  # Evaluation metric
# Predictions using XGBoost model
xgb_predictions <- predict(xgb_model, as.matrix(test_data[, -which(names(test_data) == "RICE_YIELD")]))
# Calculate performance metrics for XGBoost
xgb_mae <- mae(xgb_predictions, test_data$RICE_YIELD)
xgb_mse <- mse(xgb_predictions, test_data$RICE_YIELD)
xgb_rmse <- rmse(xgb_predictions, test_data$RICE_YIELD)
xgb_r_squared <- cor(xgb_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for XGBoost
print("XGBoost Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", xgb_mae))
print(paste("Mean Squared Error (MSE):", xgb_mse))
print(paste("Root Mean Squared Error (RMSE):", xgb_rmse))
print(paste("R-squared:", xgb_r_squared))
# Load required libraries
library(knitr)
library(ggplot2)
library(plotly)
# Create a dataframe of performance metrics
metrics_df <- data.frame(
Model = c("Random Forest", "Decision Tree", "SVM", "XGBoost"),
MAE = c(mae_rf, dt_mae, svm_mae, xgb_mae),
MSE = c(mse_rf, dt_mse, svm_mse, xgb_mse),
RMSE = c(rmse_rf, dt_rmse, svm_rmse, xgb_rmse),
R_squared = c(r_squared_rf, dt_r_squared, svm_r_squared, xgb_r_squared)
)
# Print the table
print(kable(metrics_df))
# Visualize performance metrics using ggplot2
ggplot(metrics_df, aes(x = Model, y = MAE, fill = Model)) +
geom_bar(stat = "identity") +
labs(title = "Mean Absolute Error (MAE) by Model")
# Visualize performance metrics using plotly
plot_ly(metrics_df, x = ~Model, y = ~MSE, type = "bar", color = ~Model) %>%
layout(title = "Mean Squared Error (MSE) by Model")
library(randomForest)
library(Metrics)
library(ggplot2)
# Read the data
data <- read.csv("X1.csv", header = TRUE)
y_data <- read.csv("y1.csv")
# Remove columns where all values are zero
data_filtered <- data[, colSums(data != 0) > 0]
# Combine data with target variable
data_combined <- cbind(data_filtered, RICE_YIELD = y_data)
# Set seed for reproducibility
set.seed(123)
# Sample indices for training data
train_indices <- sample(1:nrow(data_combined), 0.8 * nrow(data_combined))
# Split data into training and test sets
train_data <- data_combined[train_indices, ]
test_data <- data_combined[-train_indices, ]
# Train random forest model
rf_model <- randomForest(RICE_YIELD ~ ., data = train_data)
# Make predictions
predictions_rf <- predict(rf_model, test_data)
# Calculate performance metrics for Random Forest
mae_rf <- mae(predictions_rf, test_data$RICE_YIELD)
mse_rf <- mse(predictions_rf, test_data$RICE_YIELD)
rmse_rf <- rmse(predictions_rf, test_data$RICE_YIELD)
r_squared_rf <- cor(predictions_rf, test_data$RICE_YIELD)^2
# Print performance metrics for Random Forest
print("Random Forest Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", mae_rf))
print(paste("Mean Squared Error (MSE):", mse_rf))
print(paste("Root Mean Squared Error (RMSE):", rmse_rf))
print(paste("R-squared:", r_squared_rf))
# Train decision tree model
dt_model <- rpart(RICE_YIELD ~ ., data = train_data)
dt_predictions <- predict(dt_model, test_data)
# Calculate performance metrics for Decision Tree
dt_mae <- mae(dt_predictions, test_data$RICE_YIELD)
dt_mse <- mse(dt_predictions, test_data$RICE_YIELD)
dt_rmse <- rmse(dt_predictions, test_data$RICE_YIELD)
dt_r_squared <- cor(dt_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for Decision Tree
print("Decision Tree Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", dt_mae))
print(paste("Mean Squared Error (MSE):", dt_mse))
print(paste("Root Mean Squared Error (RMSE):", dt_rmse))
print(paste("R-squared:", dt_r_squared))
# Train SVM model
svm_model <- svm(RICE_YIELD ~ ., data = train_data)
# Predictions using SVM model
svm_predictions <- predict(svm_model, test_data)
# Calculate performance metrics for SVM
svm_mae <- mae(svm_predictions, test_data$RICE_YIELD)
svm_mse <- mse(svm_predictions, test_data$RICE_YIELD)
svm_rmse <- rmse(svm_predictions, test_data$RICE_YIELD)
svm_r_squared <- cor(svm_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for SVM
print("SVM Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", svm_mae))
print(paste("Mean Squared Error (MSE):", svm_mse))
print(paste("Root Mean Squared Error (RMSE):", svm_rmse))
print(paste("R-squared:", svm_r_squared))
# Train XGBoost model
xgb_model <- xgboost(data = as.matrix(train_data[, -which(names(train_data) == "RICE_YIELD")]),
label = train_data$RICE_YIELD,
nrounds = 100,  # Number of boosting rounds
objective = "reg:squarederror",  # Objective for regression
eval_metric = "rmse")  # Evaluation metric
# Predictions using XGBoost model
xgb_predictions <- predict(xgb_model, as.matrix(test_data[, -which(names(test_data) == "RICE_YIELD")]))
# Calculate performance metrics for XGBoost
xgb_mae <- mae(xgb_predictions, test_data$RICE_YIELD)
xgb_mse <- mse(xgb_predictions, test_data$RICE_YIELD)
xgb_rmse <- rmse(xgb_predictions, test_data$RICE_YIELD)
xgb_r_squared <- cor(xgb_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for XGBoost
print("XGBoost Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", xgb_mae))
print(paste("Mean Squared Error (MSE):", xgb_mse))
print(paste("Root Mean Squared Error (RMSE):", xgb_rmse))
print(paste("R-squared:", xgb_r_squared))
# Load required libraries
library(knitr)
library(ggplot2)
library(plotly)
# Create a dataframe of performance metrics
metrics_df <- data.frame(
Model = c("Random Forest", "Decision Tree", "SVM", "XGBoost"),
MAE = c(mae_rf, dt_mae, svm_mae, xgb_mae),
MSE = c(mse_rf, dt_mse, svm_mse, xgb_mse),
RMSE = c(rmse_rf, dt_rmse, svm_rmse, xgb_rmse),
R_squared = c(r_squared_rf, dt_r_squared, svm_r_squared, xgb_r_squared)
)
# Print the table
print(kable(metrics_df))
# Visualize performance metrics using ggplot2
ggplot(metrics_df, aes(x = Model, y = MAE, fill = Model)) +
geom_bar(stat = "identity") +
labs(title = "Mean Absolute Error (MAE) by Model")
# Visualize performance metrics using plotly
plot_ly(metrics_df, x = ~Model, y = ~MSE, type = "bar", color = ~Model) %>%
layout(title = "Mean Squared Error (MSE) by Model")
library(randomForest)
library(Metrics)
library(ggplot2)
# Read the data
data <- read.csv("X1.csv", header = TRUE)
y_data <- read.csv("y1.csv")
# Remove columns where all values are zero
data_filtered <- data[, colSums(data != 0) > 0]
# Combine data with target variable
data_combined <- cbind(data_filtered, RICE_YIELD = y_data)
# Set seed for reproducibility
set.seed(123)
# Sample indices for training data
train_indices <- sample(1:nrow(data_combined), 0.8 * nrow(data_combined))
# Split data into training and test sets
train_data <- data_combined[train_indices, ]
test_data <- data_combined[-train_indices, ]
# Train random forest model
rf_model <- randomForest(RICE_YIELD ~ ., data = train_data)
# Make predictions
predictions_rf <- predict(rf_model, test_data)
# Calculate performance metrics for Random Forest
mae_rf <- mae(predictions_rf, test_data$RICE_YIELD)
mse_rf <- mse(predictions_rf, test_data$RICE_YIELD)
rmse_rf <- rmse(predictions_rf, test_data$RICE_YIELD)
r_squared_rf <- cor(predictions_rf, test_data$RICE_YIELD)^2
# Print performance metrics for Random Forest
print("Random Forest Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", mae_rf))
print(paste("Mean Squared Error (MSE):", mse_rf))
print(paste("Root Mean Squared Error (RMSE):", rmse_rf))
print(paste("R-squared:", r_squared_rf))
# Train decision tree model
dt_model <- rpart(RICE_YIELD ~ ., data = train_data)
dt_predictions <- predict(dt_model, test_data)
# Calculate performance metrics for Decision Tree
dt_mae <- mae(dt_predictions, test_data$RICE_YIELD)
dt_mse <- mse(dt_predictions, test_data$RICE_YIELD)
dt_rmse <- rmse(dt_predictions, test_data$RICE_YIELD)
dt_r_squared <- cor(dt_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for Decision Tree
print("Decision Tree Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", dt_mae))
print(paste("Mean Squared Error (MSE):", dt_mse))
print(paste("Root Mean Squared Error (RMSE):", dt_rmse))
print(paste("R-squared:", dt_r_squared))
# Train SVM model
svm_model <- svm(RICE_YIELD ~ ., data = train_data)
# Predictions using SVM model
svm_predictions <- predict(svm_model, test_data)
# Calculate performance metrics for SVM
svm_mae <- mae(svm_predictions, test_data$RICE_YIELD)
svm_mse <- mse(svm_predictions, test_data$RICE_YIELD)
svm_rmse <- rmse(svm_predictions, test_data$RICE_YIELD)
svm_r_squared <- cor(svm_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for SVM
print("SVM Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", svm_mae))
print(paste("Mean Squared Error (MSE):", svm_mse))
print(paste("Root Mean Squared Error (RMSE):", svm_rmse))
print(paste("R-squared:", svm_r_squared))
# Train XGBoost model
xgb_model <- xgboost(data = as.matrix(train_data[, -which(names(train_data) == "RICE_YIELD")]),
label = train_data$RICE_YIELD,
nrounds = 100,  # Number of boosting rounds
objective = "reg:squarederror",  # Objective for regression
eval_metric = "rmse")  # Evaluation metric
# Predictions using XGBoost model
xgb_predictions <- predict(xgb_model, as.matrix(test_data[, -which(names(test_data) == "RICE_YIELD")]))
# Calculate performance metrics for XGBoost
xgb_mae <- mae(xgb_predictions, test_data$RICE_YIELD)
xgb_mse <- mse(xgb_predictions, test_data$RICE_YIELD)
xgb_rmse <- rmse(xgb_predictions, test_data$RICE_YIELD)
xgb_r_squared <- cor(xgb_predictions, test_data$RICE_YIELD)^2
# Print performance metrics for XGBoost
print("XGBoost Performance Metrics:")
print(paste("Mean Absolute Error (MAE):", xgb_mae))
print(paste("Mean Squared Error (MSE):", xgb_mse))
print(paste("Root Mean Squared Error (RMSE):", xgb_rmse))
print(paste("R-squared:", xgb_r_squared))
# Load required libraries
library(knitr)
library(ggplot2)
library(plotly)
# Create a dataframe of performance metrics
metrics_df <- data.frame(
Model = c("Random Forest", "Decision Tree", "SVM", "XGBoost"),
MAE = c(mae_rf, dt_mae, svm_mae, xgb_mae),
MSE = c(mse_rf, dt_mse, svm_mse, xgb_mse),
RMSE = c(rmse_rf, dt_rmse, svm_rmse, xgb_rmse),
R_squared = c(r_squared_rf, dt_r_squared, svm_r_squared, xgb_r_squared)
)
# Print the table
print(kable(metrics_df))
# Visualize performance metrics using ggplot2
ggplot(metrics_df, aes(x = Model, y = MAE, fill = Model)) +
geom_bar(stat = "identity") +
labs(title = "Mean Absolute Error (MAE) by Model")
# Visualize performance metrics using plotly
plot_ly(metrics_df, x = ~Model, y = ~MSE, type = "bar", color = ~Model) %>%
layout(title = "Mean Squared Error (MSE) by Model")
View(metrics_df)
View(data_combined)
View(data_combined)
source("~/.active-rstudio-document")
# Display the structure of the dataset
str(data_combined)
# Summary statistics of numerical variables
summary(data_combined)
# Step 1: Data Preprocessing
# Check for missing values
sum(is.na(data))
# Step 2: Feature Engineering
# You may explore relationships between features and the target variable
# Create new features if necessary
# Step 3: Model Selection and Tuning
# Random Forest model
library(randomForest)
rf_model <- randomForest(RICE_YIELD ~ ., data = data)
# Display the structure of the dataset
str(data_combined)
# Summary statistics of numerical variables
summary(data_combined)
# Step 1: Data Preprocessing
# Check for missing values
sum(is.na(data_combined))
# Step 2: Feature Engineering
# You may explore relationships between features and the target variable
# Create new features if necessary
# Step 3: Model Selection and Tuning
# Random Forest model
library(randomForest)
rf_model <- randomForest(RICE_YIELD ~ ., data_combined = data_combined)
# Display the structure of the dataset
str(data_combined)
# Summary statistics of numerical variables
summary(data_combined)
# Step 1: Data Preprocessing
# Check for missing values
sum(is.na(data_combined))
# Step 2: Feature Engineering
# You may explore relationships between features and the target variable
# Create new features if necessary
# Step 3: Model Selection and Tuning
# Random Forest model
library(randomForest)
rf_model <- randomForest(RICE_YIELD ~ ., data = data_combined)
# Decision Tree model
library(rpart)
dt_model <- rpart(RICE_YIELD ~ ., data_combined = data_combined)
# Display the structure of the dataset
str(data_combined)
# Summary statistics of numerical variables
summary(data_combined)
# Step 1: Data Preprocessing
# Check for missing values
sum(is.na(data_combined))
# Step 2: Feature Engineering
# You may explore relationships between features and the target variable
# Create new features if necessary
# Step 3: Model Selection and Tuning
# Random Forest model
library(randomForest)
rf_model <- randomForest(RICE_YIELD ~ ., data = data_combined)
# Decision Tree model
library(rpart)
dt_model <- rpart(RICE_YIELD ~ ., data = data_combined)
# SVM model
library(e1071)
svm_model <- svm(RICE_YIELD ~ ., data = data_combined)
# XGBoost model
library(xgboost)
xgb_model <- xgboost(data = as.matrix(data_combined[, -which(names(data_combined) == "RICE_YIELD")]),
label = data_combined$RICE_YIELD,
nrounds = 100,
objective = "reg:squarederror",
eval_metric = "rmse")
# Step 4: Ensemble Methods
# Combine models using ensemble techniques if necessary
# Step 5: Cross-validation
# Evaluate models using cross-validation techniques
View(y_data)
View(data_combined)
View(xgb_model)
View(y_data)
View(svm_model)
View(data_filtered)
View(data_filtered)
View(dt_model)
View(data_combined)
View(data)
View(data_filtered)
View(dt_model)
View(metrics_df)
# Read the data
data <- read.csv("your_dataset.csv", header = TRUE)
# Step 1: Data Preprocessing
# Check for missing values
sum(is.na(data_combined))
# Step 2: Feature Engineering
# (No specific feature engineering applied in this example)
# Step 3: Model Selection and Tuning
# Random Forest model
library(randomForest)
rf_model <- randomForest(RICE_YIELD ~ ., data = data_combined)
# Decision Tree model
library(rpart)
dt_model <- rpart(RICE_YIELD ~ ., data = data_combined)
# SVM model
library(e1071)
svm_model <- svm(RICE_YIELD ~ ., data = data_combined)
# XGBoost model
library(xgboost)
xgb_model <- xgboost(data = as.matrix(data_combined[, -which(names(data_combined) == "RICE_YIELD")]),
label = data_combined$RICE_YIELD,
nrounds = 100,
objective = "reg:squarederror",
eval_metric = "rmse")
# Step 4: Ensemble Methods
# Combine models using ensemble techniques if necessary
# Step 5: Cross-validation
# Evaluate models using cross-validation techniques
# Step 6: Model Evaluation
# Random Forest
rf_predictions <- predict(rf_model, data)
rf_mae <- mean(abs(rf_predictions - data$RICE_YIELD))
rf_mse <- mean((rf_predictions - data$RICE_YIELD)^2)
rf_rmse <- sqrt(mean((rf_predictions - data$RICE_YIELD)^2))
rf_r_squared <- cor(rf_predictions, data$RICE_YIELD)^2
# Read the data
# Step 1: Data Preprocessing
# Check for missing values
sum(is.na(data_combined))
# Step 2: Feature Engineering
# (No specific feature engineering applied in this example)
# Step 3: Model Selection and Tuning
# Random Forest model
library(randomForest)
rf_model <- randomForest(RICE_YIELD ~ ., data = data_combined)
# Decision Tree model
library(rpart)
dt_model <- rpart(RICE_YIELD ~ ., data = data_combined)
# SVM model
library(e1071)
svm_model <- svm(RICE_YIELD ~ ., data = data_combined)
# XGBoost model
library(xgboost)
xgb_model <- xgboost(data = as.matrix(data_combined[, -which(names(data_combined) == "RICE_YIELD")]),
label = data_combined$RICE_YIELD,
nrounds = 100,
objective = "reg:squarederror",
eval_metric = "rmse")
# Step 4: Ensemble Methods
# Combine models using ensemble techniques if necessary
# Step 5: Cross-validation
# Evaluate models using cross-validation techniques
# Step 6: Model Evaluation
# Random Forest
rf_predictions <- predict(rf_model, data_combined)
rf_mae <- mean(abs(rf_predictions - data_combined$RICE_YIELD))
rf_mse <- mean((rf_predictions - data_combined$RICE_YIELD)^2)
rf_rmse <- sqrt(mean((rf_predictions - data_combined$RICE_YIELD)^2))
rf_r_squared <- cor(rf_predictions, data_combined$RICE_YIELD)^2
# Decision Tree
dt_predictions <- predict(dt_model, data_combined)
dt_mae <- mean(abs(dt_predictions - data_combined$RICE_YIELD))
dt_mse <- mean((dt_predictions - data_combined$RICE_YIELD)^2)
dt_rmse <- sqrt(mean((dt_predictions - data_combined$RICE_YIELD)^2))
dt_r_squared <- cor(dt_predictions, data_combined$RICE_YIELD)^2
# SVM
svm_predictions <- predict(svm_model, data_combined)
svm_mae <- mean(abs(svm_predictions - data_combined$RICE_YIELD))
svm_mse <- mean((svm_predictions - data_combined$RICE_YIELD)^2)
svm_rmse <- sqrt(mean((svm_predictions - data_combined$RICE_YIELD)^2))
svm_r_squared <- cor(svm_predictions, data_combined$RICE_YIELD)^2
# XGBoost
xgb_predictions <- predict(xgb_model, as.matrix(data_combined[, -which(names(data_combined) == "RICE_YIELD")]))
xgb_mae <- mean(abs(xgb_predictions - data_combined$RICE_YIELD))
xgb_mse <- mean((xgb_predictions - data_combined$RICE_YIELD)^2)
xgb_rmse <- sqrt(mean((xgb_predictions - data_combined$RICE_YIELD)^2))
xgb_r_squared <- cor(xgb_predictions, data_combined$RICE_YIELD)^2
# Displaying metrics
results <- data.frame(
Model = c("Random Forest", "Decision Tree", "SVM", "XGBoost"),
MAE = c(rf_mae, dt_mae, svm_mae, xgb_mae),
MSE = c(rf_mse, dt_mse, svm_mse, xgb_mse),
RMSE = c(rf_rmse, dt_rmse, svm_rmse, xgb_rmse),
R_squared = c(rf_r_squared, dt_r_squared, svm_r_squared, xgb_r_squared)
)
print(results)
save(xgb_model, file = "xgb_model")
save(xgb_model, file = "xgb_model.RData")
source("C:/Users/sumit/Desktop/Data Science Practical/CP/Rice-Yield-Prediction/DEMO.R")
